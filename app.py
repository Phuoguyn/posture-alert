# -*- coding: utf-8 -*-
"""ML_project_complete_after_change

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LDTxdnmPy3AablF8bDBemTTuf5o10_bz
"""

#appDomain

#This project is situated in the domain of digital wellness and ergonomic support, focusing on maintaining healthy posture during extended computer use. It helps students develop posture awareness through real-time webcam feedback and gentle corrective prompts.

#app.py
# Typing Posture Alert (Streamlit + MediaPipe + streamlit-webrtc)
# Features:
#   ‚Ä¢ Consent gate
#   ‚Ä¢ Webcam posture detection (neck/back/head)
#   ‚Ä¢ Personal calibration (baseline angles)
#   ‚Ä¢ On-screen coaching alerts for bad posture
#   ‚Ä¢ Bad-posture > 60s alert + ‚Äúmove your body‚Äù every 30 minutes
#   ‚Ä¢ Analytics dashboard (posture % and timeline)
#
# Works on desktop and mobile (browser webcam). For real-time streaming we use streamlit-webrtc.


import math
import time
import collections
from dataclasses import dataclass, field
from typing import Deque, Dict, List, Optional, Tuple



import av
import cv2
import numpy as np
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase

import mediapipe as mp


from PyQt6 import QtCore, QtGui, QtWidgets

st.set_page_config(page_title="Typing Posture Alert", page_icon="üßç", layout="wide")

# ---- GLOBAL CONSTANTS ----

# Default thresholds (used before calibration)
NECK_TILT_MAX_DEFAULT = 25.0     # deg, larger => slouch
BACK_ANGLE_MIN_DEFAULT = 150.0   # deg, smaller => slouch
HEAD_PITCH_MAX_DEFAULT = 20.0    # deg, larger => slouch

SMOOTH_WINDOW = 30               # frames to smooth decisions (rolling window)
BAD_POSTURE_PERSIST_SEC = 60.0   # if bad posture persists > 1 minute => strong alert
MOVE_BODY_INTERVAL_SEC = 30 * 60 # every 30 minutes, remind to move

# UI scaling for mobile (keep it simple/large)
MOBILE_MAX_WIDTH = 720

# ---- SESSION STATE -------

def ss_get(key, default): #session state
    if key not in st.session_state: #check if that vars already exists
        st.session_state[key] = default #if key missing creates it again
    return st.session_state[key] #returns the value

#save values (val) into steamlit session memory (st.session_state) using key
def ss_set(key, val):
    st.session_state[key] = val

# Initialize session state
ss_get("consented", False) #need users consent
ss_get("calibrated", False) #avoid postur detection run before baseline posture is set
ss_get("baseline_angles", {"neck": None, "back": None, "head": None}) #empty until the calibration is complete
ss_get("thresholds", {
    "NECK_TILT_MAX": NECK_TILT_MAX_DEFAULT,
    "BACK_ANGLE_MIN": BACK_ANGLE_MIN_DEFAULT,
    "HEAD_PITCH_MAX": HEAD_PITCH_MAX_DEFAULT
}) # how far u can move before being flag
ss_get("stats", {
    "start_ts": None,           # session start time
    "last_good_ts": None,       # last time posture was good
    "last_move_reminder": None, # last time we reminded to move
    "bad_streak_start": None,   # when a continuous bad posture streak began
    "timeline": [],             # list of (t_rel_seconds, is_good)
    "good_frames": 0,
    "bad_frames": 0,
}) #this is a dictionary use to track al posture stats
ss_get("show_landmarks", True)
ss_get("mirror_video", True)
ss_get("sensitivity", 1.0)  # scale thresholds post-calibration, move up if need more sensitivity

# ---- POSE / GEOMETRY ----

def _unit(v: np.ndarray) -> np.ndarray: #helper funs _unit
  n = np.linalg.norm(v)
  return v if n == 0 else v / n #return numpy array

def angle_deg(v1: np.ndarray, v2: np.ndarray) -> float:
    v1u, v2u = _unit(v1), _unit(v2)
    dot = float(np.clip(np.dot(v1u, v2u), -1.0, 1.0))
    return float(np.degrees(np.arccos(dot)))

from typing import Deque, Dict, List, Optional, Tuple

#this is the shorter version than before
VERTICAL = np.array([0.0, -1.0])

def vec(pA, pB, W, H) -> np.ndarray:
    return np.array([(pB.x - pA.x) * W, (pB.y - pA.y) * H], dtype=float)

def features_from_landmarks(lm, W: int, H: int) -> Tuple[float, float, float]:
    """
    Returns (neck_tilt, back_angle, head_pitch) in degrees.
    Uses MediaPipe Pose indices: 11,12=shoulders; 23,24=hips; 0=nose
    """
    ls, rs, lh, rh, nose = lm[11], lm[12], lm[23], lm[24], lm[0]
    make_point = lambda x, y: type('P', (), dict(x=x, y=y))()

    ear_like = make_point((ls.x + nose.x) / 2, (ls.y + nose.y) / 2)
    neck_tilt  = angle_deg(vec(ls, ear_like, W, H), VERTICAL)
    back_angle = angle_deg(vec(lh, ls, W, H), VERTICAL)
    mid = make_point((ls.x + rs.x) / 2, (ls.y + rs.y) / 2)
    head_pitch = angle_deg(vec(mid, nose, W, H), VERTICAL)

    return neck_tilt, back_angle, head_pitch

# ---- VIDEO PROCESSOR ----

from dataclasses import dataclass, field

@dataclass
class PostureState:
    hist: Deque[int] = field(default_factory=lambda: collections.deque(maxlen=SMOOTH_WINDOW))
    last_frame_ts: float = 0.0

class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        self.pose = mp.solutions.pose.Pose(
            min_detection_confidence=0.5, min_tracking_confidence=0.5
        )
        self.draw = mp.solutions.drawing_utils
        self.styles = mp.solutions.drawing_styles
        self.state = PostureState()

    def _get_thresholds(self) -> Dict[str, float]:
        th = st.session_state["thresholds"].copy()
        # Apply sensitivity scaling (e.g., 0.8 = stricter, 1.2 = looser)
        s = float(st.session_state.get("sensitivity", 1.0))
        th["NECK_TILT_MAX"] = th["NECK_TILT_MAX"] * (1.0 / s)   # stricter if s>1
        th["HEAD_PITCH_MAX"] = th["HEAD_PITCH_MAX"] * (1.0 / s)
        th["BACK_ANGLE_MIN"] = th["BACK_ANGLE_MIN"] * (1.0 / s)
        return th

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        H, W = img.shape[:2]

        # Mirror for a more natural selfie view if enabled
        if st.session_state.get("mirror_video", True):
            img = cv2.flip(img, 1)

        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        res = self.pose.process(rgb)

        neck = back = head = None
        is_slouch = False
        if res.pose_landmarks:
            lm = res.pose_landmarks.landmark
            neck, back, head = features_from_landmarks(lm, W, H)

            # Draw landmarks if user wants
            if st.session_state.get("show_landmarks", True):
                self.draw.draw_landmarks(
                    img, res.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=self.styles.get_default_pose_landmarks_style()
                )

            # Compare to thresholds
            th = self._get_thresholds()
            bad_neck = (neck is not None and neck > th["NECK_TILT_MAX"])
            bad_back = (back is not None and back < th["BACK_ANGLE_MIN"])
            bad_head = (head is not None and head > th["HEAD_PITCH_MAX"])

            is_slouch = bad_neck or bad_back or bad_head

            # Coaching message (pick strongest offender)
            msg = ""
            if is_slouch:
                offenders = []
                if bad_back: offenders.append(("back", abs(back - th["BACK_ANGLE_MIN"])))
                if bad_head: offenders.append(("head", abs(head - th["HEAD_PITCH_MAX"])))
                if bad_neck: offenders.append(("neck", abs(neck - th["NECK_TILT_MAX"])))
                offenders.sort(key=lambda x: x[1], reverse=True)
                top = offenders[0][0] if offenders else None
                if top == "back":
                    msg = "Adjust your back (sit upright)."
                elif top == "head":
                    msg = "Bring your head back (reduce forward head)."
                elif top == "neck":
                    msg = "Relax neck; stack ears over shoulders."
                else:
                    msg = "Adjust posture (sit tall)."
            else:
                msg = "Good posture üëç"

            # Overlay numbers + message
            y = 28
            def put(txt, color=(0,255,0)):
                nonlocal y
                cv2.putText(img, txt, (10,y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)
                y += 28

            if neck is not None:
                put(f"Neck tilt : {neck:5.1f}¬∞")
                put(f"Back angle: {back:5.1f}¬∞")
                put(f"Head pitch: {head:5.1f}¬∞")

            if is_slouch:
                cv2.rectangle(img, (0,0), (W,60), (0,0,255), -1)
                cv2.putText(img, msg + "  (1-min timer running)", (10,42),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.85, (255,255,255), 2, cv2.LINE_AA)
            else:
                cv2.rectangle(img, (0,0), (W,40), (0,128,0), -1)
                cv2.putText(img, msg, (10,28),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.85, (255,255,255), 2, cv2.LINE_AA)

        # ---- Update stats / analytics ----
        now = time.time()
        stats = st.session_state["stats"]
        if stats["start_ts"] is None:
            stats["start_ts"] = now
            stats["last_move_reminder"] = now

        # Smoothing
        self.state.hist.append(1 if is_slouch else 0)
        slouch_ratio = sum(self.state.hist) / max(1, len(self.state.hist))
        is_good = (slouch_ratio < 0.5)

        # Frame counters (approximate, FPS-independent aggregation)
        if is_good:
            stats["good_frames"] += 1
            stats["last_good_ts"] = now
        else:
            stats["bad_frames"] += 1

        # Continuous bad posture streak timing
        if not is_good:
            if stats["bad_streak_start"] is None:
                stats["bad_streak_start"] = now
            else:
                if (now - stats["bad_streak_start"]) >= BAD_POSTURE_PERSIST_SEC:
                    # Strong alert (Toast)
                    st.toast("‚è∞ Bad posture for 1 minute ‚Äî please reset posture.", icon="‚ö†Ô∏è")
                    stats["bad_streak_start"] = now  # reset the one-minute window
        else:
            stats["bad_streak_start"] = None

        # Move-your-body every 30 minutes
        if stats["last_move_reminder"] is not None and (now - stats["last_move_reminder"]) >= MOVE_BODY_INTERVAL_SEC:
            st.toast("üïí Time to move your body for a minute!", icon="‚è≥")
            stats["last_move_reminder"] = now

        # Timeline (every ~2 seconds to keep it light)
        if (now - self.state.last_frame_ts) > 2.0:
            t_rel = now - stats["start_ts"]
            stats["timeline"].append((t_rel, 1 if is_good else 0))
            self.state.last_frame_ts = now

        # Ship processed frame out
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ---- CALIBRATION --------

def run_calibration(seconds: int = 5):
    """
    Collects a few seconds of 'good posture' frames and sets personalized thresholds.
    """
    st.info("Sit in your best posture. Calibration will average your angles and set personalized thresholds.")
    th0 = {
        "NECK_TILT_MAX": NECK_TILT_MAX_DEFAULT,
        "BACK_ANGLE_MIN": BACK_ANGLE_MIN_DEFAULT,
        "HEAD_PITCH_MAX": HEAD_PITCH_MAX_DEFAULT,
    }

    # Use a tiny temporary processor to read angles for a few seconds
    mp_pose = mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    draw = mp.solutions.drawing_utils

    st.write("Click **Start camera** below, then hold still in good posture.")
    calib_ctx = webrtc_streamer(
        key="calibration",
        mode=WebRtcMode.SENDRECV,
        media_stream_constraints={"video": True, "audio": False},
        video_processor_factory=None,  # raw frames
    )

    readings = []
    start = None
    progress = st.progress(0.0, text="Waiting for camera...")
    while calib_ctx.state.playing:
        frame = calib_ctx.video_receiver.get_frame(timeout=1)
        if frame is None:
            continue
        img = frame.to_ndarray(format="bgr24")
        H, W = img.shape[:2]
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        res = mp_pose.process(rgb)
        if res.pose_landmarks:
            lm = res.pose_landmarks.landmark
            neck, back, head = features_from_landmarks(lm, W, H)
            readings.append((neck, back, head))
            if start is None:
                start = time.time()
            elapsed = time.time() - start
            progress.progress(min(1.0, elapsed/seconds), text=f"Calibrating... {min(seconds, int(elapsed))}/{seconds}s")
            if elapsed >= seconds:
                break

    # Compute personalized thresholds
    if readings:
        arr = np.array(readings)  # shape (n, 3)
        neck_mean, back_mean, head_mean = arr.mean(axis=0).tolist()
        ss_set("baseline_angles", {"neck": neck_mean, "back": back_mean, "head": head_mean})

        # Set thresholds relative to baseline with safe margins
        personalized = {
            "NECK_TILT_MAX": max(10.0, neck_mean + 10.0),  # allow +10¬∞ from your best
            "BACK_ANGLE_MIN": min(179.0, back_mean - 10.0), # allow -10¬∞ from your best
            "HEAD_PITCH_MAX": max(8.0, head_mean + 8.0),    # allow +8¬∞ from your best
        }
        ss_set("thresholds", personalized)
        ss_set("calibrated", True)
        st.success("Calibration complete! Personalized thresholds set.")
    else:
        st.warning("No pose detected during calibration. You can skip calibration or try again.")

# ---- UI LAYOUT ---------

def consent_gate():
    st.title("üßç Typing Posture Alert")
    st.markdown(
        """
        **This app uses your webcam in your browser** to analyze your posture locally (no video is uploaded).
        Please confirm:
        - I allow the app to access my webcam.
        - I understand posture analysis runs in this session only and is not stored.
        """
    )
    agree = st.checkbox("I consent to webcam access and local posture analysis.")
    if st.button("Continue", type="primary", disabled=not agree):
        ss_set("consented", True)
        st.rerun()

def monitor_section():
    st.header("Live Monitor")
    colL, colR = st.columns([2,1])
    with colL:
        st.toggle("Mirror video", value=st.session_state["mirror_video"], key="mirror_video")
        st.toggle("Show landmarks", value=st.session_state["show_landmarks"], key="show_landmarks")
        st.slider("Sensitivity (higher = stricter)", 0.7, 1.5, key="sensitivity", value=1.0, step=0.05)
        webrtc_ctx = webrtc_streamer(
            key="posture-monitor",
            mode=WebRtcMode.SENDRECV,
            media_stream_constraints={"video": True, "audio": False},
            video_processor_factory=VideoProcessor,
        )

    with colR:
        st.subheader("Analytics")
        stats = st.session_state["stats"]
        start_ts = stats["start_ts"]
        good = stats["good_frames"]
        bad = stats["bad_frames"]
        total = max(1, good + bad)
        pct = 100.0 * good / total
        st.metric("Posture % Good", f"{pct:.1f}%")
        st.caption("Percentage of frames assessed as 'good' during this session.")

        # Timeline chart
        tl = stats["timeline"]
        if len(tl) >= 2:
            import pandas as pd
            df = pd.DataFrame(tl, columns=["t_sec", "good"])
            df["minutes"] = df["t_sec"] / 60.0
            df.set_index("minutes", inplace=True)
            st.line_chart(df["good"], height=160, use_container_width=True)
            st.caption("Timeline: 1 = good posture, 0 = slouching (sampled ~every 2s).")
        else:
            st.info("Timeline will appear after ~10 seconds of monitoring.")

        # Quick stats
        if start_ts:
            elapsed_min = (time.time() - start_ts) / 60.0
            st.metric("Session length", f"{elapsed_min:.1f} min")

        st.divider()
        if st.button("Reset analytics"):
            ss_set("stats", {
                "start_ts": None,
                "last_good_ts": None,
                "last_move_reminder": None,
                "bad_streak_start": None,
                "timeline": [],
                "good_frames": 0,
                "bad_frames": 0,
            })
            st.toast("Analytics reset.", icon="üîÑ")
            st.rerun()

    st.info("Coaching: If you see red banner, adjust your **back / head / neck or distance**. "
            "If bad posture persists for 1 minute, you‚Äôll get a strong reminder. "
            "Every 30 minutes, you‚Äôll be nudged to move your body.")


def sidebar_info():
    with st.sidebar:
        st.header("Calibration")
        if not st.session_state["calibrated"]:
            st.button("Run 5s calibration", on_click=lambda: None, key="calib_enabler")
            if st.session_state.get("calib_enabler_clicked_once") is None:
                st.session_state["calib_enabler_clicked_once"] = True
            run_button = st.button("Start calibration now")
            if run_button:
                run_calibration(5)
        else:
            base = st.session_state["baseline_angles"]
            th = st.session_state["thresholds"]
            st.success("Calibrated ‚úÖ")
            st.write("**Baseline angles (your best):**")
            st.json({k: round(v,1) if v else None for k,v in base.items()})
            st.write("**Personalized thresholds:**")
            st.json({k: round(v,1) for k,v in th.items()})
            if st.button("Re-calibrate"):
                ss_set("calibrated", False)

        st.divider()
        st.subheader("Privacy")
        st.caption("‚Ä¢ Video stays in your browser session.\n"
                   "‚Ä¢ No dataset is collected or stored.\n"
                   "‚Ä¢ You can close the tab to end the session.")

# ---- MAIN APP FLOW ------

def main():
    # Mobile width hint
    st.markdown(
        f"""
        <style>
        .block-container {{ max-width: {MOBILE_MAX_WIDTH}px; }}
        </style>
        """,
        unsafe_allow_html=True
    )

    if not st.session_state["consented"]:
        consent_gate()
        return

    # Content
    st.title("üßç Typing Posture Alert")
    st.caption("Webcam + MediaPipe detects slouching posture and gently alerts. "
               "Calibrate once, then monitor your study session.")

    sidebar_info()
    monitor_section()

if __name__ == "__main__":
    main()


